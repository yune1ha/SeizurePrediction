{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f781b2da-c57f-44fd-a26e-3adb9b26ade7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReferences\\n# Model averaging: \\n* https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/\\n# Activations: \\n* https://github.com/siebenrock/activation-functions\\n* https://www.v7labs.com/blog/neural-networks-activation-functions\\n# XAI:\\n* https://towardsdatascience.com/deep-learning-model-interpretation-using-shap -a21786e91d16\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "README\n",
    "# python 3.8\n",
    "# coding: utf-8\n",
    "# main script for /modelers\n",
    "'''\n",
    "\n",
    "#########################################################################################\n",
    "'''\n",
    "References\n",
    "# Model averaging: \n",
    "* https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/\n",
    "# Activations: \n",
    "* https://github.com/siebenrock/activation-functions\n",
    "* https://www.v7labs.com/blog/neural-networks-activation-functions\n",
    "# XAI:\n",
    "* https://towardsdatascience.com/deep-learning-model-interpretation-using-shap -a21786e91d16\n",
    "'''\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0587098e-d41b-4e9d-9765-6e25809e42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Flatten, BatchNormalization, \n",
    "    Conv2D, MaxPooling2D, GlobalAveragePooling2D,\n",
    "    Activation)\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "# Settings\n",
    "kernel_size = (1,3)\n",
    "\n",
    "# Knife\n",
    "def shaper(Xs):            \n",
    "    X_train, X_valid, X_test = Xs\n",
    "    print('in Xs:', X_train.shape, X_valid.shape, X_test.shape)\n",
    "    \n",
    "    # Scale (for 3D shaped X)            \n",
    "    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "    X_valid = scaler.transform(X_valid.reshape(-1, X_valid.shape[-1])).reshape(X_valid.shape)\n",
    "    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "    rows, cols = X_train.shape[1], X_train.shape[2]    \n",
    "    input_shape = (rows, cols, 1) \n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape + (1,)).astype('float32')\n",
    "    X_valid = X_valid.reshape(X_valid.shape + (1,)).astype('float32')\n",
    "    X_test = X_test.reshape(X_test.shape + (1,)).astype('float32')\n",
    "    \n",
    "    Xs_out = (X_train, X_valid, X_test)\n",
    "    return Xs_out, input_shape\n",
    "            \n",
    "def bsdcnn_relu(Xs, learning_rate, batch_size):  \n",
    "    print('Initiated bsdcnn.py...')\n",
    "    \n",
    "    # plastic surgery\n",
    "    Xs_out, input_shape = shaper(Xs)    \n",
    "    \n",
    "    # pick 1 model\n",
    "    model = b1_mlp(input_shape, batch_size)\n",
    "    \n",
    "    opt_adam = tf.keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=opt_adam, \n",
    "                  metrics=['binary_accuracy'],\n",
    "                 )\n",
    "    \n",
    "    return Xs_out, model \n",
    "\n",
    "#################################################################################\n",
    "## Builds\n",
    "\n",
    "def b1_bsdcnn(input_shape, batch_size):\n",
    "    # https://www.researchgate.net/publication/349293613_Binary_Single-Dimensional_Convolutional_Neural_Network_for_Seizure_Prediction\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, (1,5), activation='relu',input_shape=input_shape))\n",
    "    model.add(Conv2D(16, (1,5), strides=2, activation='relu',padding=\"valid\"))\n",
    "    model.add(BatchNormalization())  \n",
    "    \n",
    "    model.add(Conv2D(32, (1,5), padding=\"same\", activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (1,5), strides=2, padding=\"same\", activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(64, (1,10), padding=\"same\", activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (1,10), strides=2, padding=\"same\", activation='relu'))\n",
    "    model.add(BatchNormalization())   \n",
    "    \n",
    "    model.add(Conv2D(128, (2,1), padding=\"same\", activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (2,1), strides=2 ,padding=\"same\", activation='relu'))\n",
    "    model.add(BatchNormalization())  \n",
    "    \n",
    "    model.add(Conv2D(256, (2,1), padding=\"same\", activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (2,1), strides=2, padding=\"same\", activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='sigmoid'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation='sigmoid'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cc11d1-2796-4a23-a5e3-0f5968359cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Flatten, BatchNormalization, \n",
    "    Conv2D, MaxPooling2D, GlobalAveragePooling2D,\n",
    "    Activation)\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "# Settings\n",
    "kernel_size = (1,3)\n",
    "\n",
    "# Knife\n",
    "def shaper(Xs):            \n",
    "    X_train, X_valid, X_test = Xs\n",
    "    print('in Xs:', X_train.shape, X_valid.shape, X_test.shape)\n",
    "    \n",
    "    # Scale (for 3D shaped X)            \n",
    "    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "    X_valid = scaler.transform(X_valid.reshape(-1, X_valid.shape[-1])).reshape(X_valid.shape)\n",
    "    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "    rows, cols = X_train.shape[1], X_train.shape[2]    \n",
    "    input_shape = (rows, cols, 1) \n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape + (1,)).astype('float32')\n",
    "    X_valid = X_valid.reshape(X_valid.shape + (1,)).astype('float32')\n",
    "    X_test = X_test.reshape(X_test.shape + (1,)).astype('float32')\n",
    "    \n",
    "    Xs_out = (X_train, X_valid, X_test)\n",
    "    return Xs_out, input_shape\n",
    "            \n",
    "def mlp(Xs, learning_rate, batch_size):  \n",
    "    print('Initiated bsdcnn.py...')\n",
    "    \n",
    "    # plastic surgery\n",
    "    Xs_out, input_shape = shaper(Xs)    \n",
    "    \n",
    "    # pick 1 model\n",
    "    model = b1_mlp(input_shape, batch_size)\n",
    "    \n",
    "    opt_adam = tf.keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=opt_adam, \n",
    "                  metrics=['binary_accuracy'],\n",
    "                 )\n",
    "    \n",
    "    return Xs_out, model \n",
    "\n",
    "#################################################################################\n",
    "## Builds\n",
    "\n",
    "def b1_mlp(input_shape, batch_size):\n",
    "    # https://www.researchgate.net/publication/349293613_Binary_Single-Dimensional_Convolutional_Neural_Network_for_Seizure_Prediction\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(batch_size, input_dim=input_shape, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(batch_size*4, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(batch_size, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(batch_size/4, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e580721c-778a-4afe-9a07-fac0ef70e076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.utils import plot_model, to_categorical # unused aon\n",
    "from tensorflow.keras.metrics import *\n",
    "\n",
    "#from keras_visualizer import visualizer\n",
    "#from model_profiler import model_profiler\n",
    "\n",
    "import shap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from numba import cuda\n",
    "import os, random, copy, warnings, math, time, csv\n",
    "from pathlib import Path\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90203d84-1864-4fed-9396-39e8d42d1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "arr_dir = '/home/SharedFiles/Projects/EEG/Inputs/seq_arr/'\n",
    "outputs_dir = '/home/SharedFiles/Projects/EEG/Outputs/runs'\n",
    "\n",
    "#########################################################################################\n",
    "# Models ### CHECKER\n",
    "engine_names = ('BSDCNN_relu',) #('CNN1Db2', 'CNN2Db1', 'BSDCNNb1')\n",
    "engines = (bsdcnn_relu,) #(cnn1d, cnn2d, bsdcnn)\n",
    "\n",
    "# engine_names = ('CNN1Db2', 'CNN2Db1', 'BSDCNNb1') \n",
    "# engines = (cnn1d, cnn2d, bsdcnn)\n",
    "\n",
    "# channel picks  ### CHECKER\n",
    "ch_picks_list = ('all',) #('rt', 'lt') \n",
    "\n",
    "all_ch = ('Fp1-AVG', 'F3-AVG', 'C3-AVG','P3-AVG','Fp2-AVG', 'F4-AVG', 'C4-AVG','P4-AVG','F7-AVG',\n",
    "              'T1-AVG', 'T3-AVG', 'T5-AVG', 'O1-AVG', 'F8-AVG', 'T2-AVG', 'T4-AVG', 'T6-AVG', 'O2-AVG',\n",
    "              'Fz-AVG', 'Cz-AVG', 'Pz-AVG') # in order of index!\n",
    "#ch_picks_list += all_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4249061-cc5b-4aa6-90cb-ce14865f62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Settings\n",
    "learning_rate = 1e-5 \n",
    "batch_size = 64 # CHECKER\n",
    "epochs = 1 #10** 2 \n",
    "patience = 10\n",
    "\n",
    "# Dataset settings\n",
    "train_ratio = 0.7\n",
    "\n",
    "# Other settings\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6fbc93-7613-41ee-ad4a-7f6446b49669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetcher(_dir):\n",
    "    patients, all_preictals, all_interictals = [], [], []\n",
    "    for patient in os.listdir(_dir):\n",
    "        if patient.startswith('SNUCH01'): # CHECKER\n",
    "            #print('patient', patient) # TEST\n",
    "            patient_pth = os.path.join(_dir, patient)\n",
    "            patients.append(patient)\n",
    "            all_preictals.append([])\n",
    "            all_interictals.append([])\n",
    "            for ictalType in os.listdir(patient_pth):\n",
    "                ictalType_pth = os.path.join(patient_pth, ictalType) \n",
    "                if os.path.isdir(ictalType_pth):\n",
    "                    if ictalType == 'preictals':\n",
    "                        for preictal in os.listdir(ictalType_pth):\n",
    "                            if not preictal.startswith('.'):\n",
    "                                preictal_pth = os.path.join(ictalType_pth, preictal)\n",
    "                                all_preictals[-1].append(preictal_pth)\n",
    "                    if ictalType == 'interictals':\n",
    "                        for interictal in os.listdir(ictalType_pth):\n",
    "                            if not interictal.startswith('.'):\n",
    "                                interictal_pth = os.path.join(ictalType_pth, interictal)\n",
    "                                all_interictals[-1].append(interictal_pth)\n",
    "\n",
    "    print('Num patients:', len(patients))\n",
    "    print('Cnt equivalency:', len(patients) == len(all_preictals) == len(all_interictals))\n",
    "    \n",
    "    # zip and order\n",
    "    zipper = list(zip(patients, all_preictals, all_interictals))\n",
    "    zipper = zip(*sorted(zipper, key = lambda x:x[0]))\n",
    "     \n",
    "    return tuple(zipper)\n",
    "\n",
    "\n",
    "# train valid test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f3c211-e6fe-4d4e-9fda-f7bdb233196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(preictals, interictals, train_ratio, ch_picks, t):\n",
    "    # # shape test\n",
    "    # prime = np.load(preictals[0]).shape\n",
    "    # print('preictals shape:', prime)\n",
    "    # print([np.load(ele).shape for ele in preictals if np.load(ele).shape != np.load(preictals[0]).shape])\n",
    "    # prime = np.load(interictals[0]).shape\n",
    "    # print('interictals shape:', prime)\n",
    "    # print([np.load(ele).shape for ele in interictals if np.load(ele).shape != np.load(interictals[0]).shape])\n",
    "    \n",
    "    tot_timesteps = 3000 ### hardcoded\n",
    "    \n",
    "    if ch_picks == 'all':\n",
    "        mask = [i for i in range(len(all_ch))] # find alternative\n",
    "            \n",
    "#     elif ch_picks == 'rt':\n",
    "#         rt_ch = ('Fp2-AVG', 'F4-AVG', 'C4-AVG', 'P4-AVG', 'O2-AVG', 'F8-AVG','T4-AVG', 'T6-AVG')\n",
    "#         mask = [all_ch.index(ch) for ch in rt_ch]\n",
    "        \n",
    "#     elif ch_picks == 'lt':\n",
    "#         lt_ch = ('Fp1-AVG', 'F3-AVG', 'C3-AVG','P3-AVG', 'O1-AVG' ,'F7-AVG', 'T3-AVG', 'T5-AVG')\n",
    "#         mask = [all_ch.index(ch) for ch in lt_ch]\n",
    "        \n",
    "#     else:\n",
    "#         single_ch = ch_picks\n",
    "#         print('Single channel:', single_ch)\n",
    "#         print('Single channel:', single_ch, file=t)\n",
    "#         mask = [all_ch.index(single_ch)]\n",
    "        \n",
    "        \n",
    "    X = [np.load(npy)[mask,:tot_timesteps] if np.load(npy).shape[1] >tot_timesteps else np.load(npy)[mask,:] for npy in preictals] + \\\n",
    "        [np.load(npy)[mask,:tot_timesteps] if np.load(npy).shape[1] >tot_timesteps else np.load(npy)[mask,:] for npy in interictals]   \n",
    "    y = [1. for it in preictals] + [0. for it in interictals]\n",
    "    \n",
    "    data = list(zip(X, y))\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    valid_size = int((len(data) - train_size) / 2) # 1:1 = valid:test\n",
    "    valid_ind = train_size + valid_size\n",
    "    \n",
    "    train = data[:train_size]\n",
    "    valid = data[train_size:valid_ind]\n",
    "    test = data[valid_ind:]\n",
    "    \n",
    "    X_train, y_train = zip(*train)\n",
    "    X_valid, y_valid = zip(*valid)\n",
    "    X_test, y_test = zip(*test)\n",
    "              \n",
    "    return np.array(X_train, dtype=np.float32), np.array(y_train), np.array(X_valid, dtype=np.float32), np.array(y_valid), np.array(X_test, dtype=np.float32), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3a2abb5-4b3e-4097-bae9-64ff7661df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceVisualizationCallback(Callback):\n",
    "    def __init__(self, model, validation_data, image_dir):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.validation_data = validation_data\n",
    "        \n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        y_true = self.validation_data[1]             \n",
    "        y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # plot and save confusion matrix\n",
    "        fig, ax = plt.subplots(figsize=(16,12))\n",
    "        plot_confusion_matrix(y_true, y_pred_class, ax=ax)\n",
    "        # fig.savefig(os.path.join(self.image_dir, f'confusion_matrix_epoch_{epoch}'))\n",
    "\n",
    "       # plot and save roc curve\n",
    "        fig, ax = plt.subplots(figsize=(16,12))\n",
    "        plot_roc(y_true, y_pred, ax=ax)\n",
    "        # fig.savefig(os.path.join(self.image_dir, f'roc_curve_epoch_{epoch}'))\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66cf6ba4-6d6c-484f-b865-59d098849f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model, g_X_train, g_y_train, g_X_test, g_y_test = None,None,None,None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5ed4780-eb69-41c8-8604-6febaf435c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeler(arr_name, patients, all_preictals, all_interictals, t, cw, run_name, run_dir, engine_name, engine, ch_picks):\n",
    "    # paths\n",
    "    image_dir = os.path.join(run_dir, 'visuals')\n",
    "    model_dir = os.path.join(run_dir, 'models')\n",
    "\n",
    "    # Multi-GPU Set-up # https://keras.io/guides/distributed_training/\n",
    "    strategy = tf.distribute.MirroredStrategy() # multi-GPU\n",
    "    print('Num devices: {}'.format(strategy.num_replicas_in_sync)) # num GPUs\n",
    "    #print('Patience:', patience)\n",
    "    #print('Learning rate:', learning_rate)\n",
    "    print('Batch size:', batch_size)\n",
    "\n",
    "    print()\n",
    "    print('-' * 65)\n",
    "\n",
    "    # all auc plot\n",
    "    plt.figure(2)\n",
    "    plt.title('Model AUC (All Patients)')\n",
    "    plt.axis([0,1,0,1])\n",
    "    plt.ylabel('TPR')\n",
    "    plt.xlabel('FPR')\n",
    "\n",
    "    all_auc_keras, all_loss, all_acc = [], [], []\n",
    "\n",
    "    for idx, patient in enumerate(patients[:1]): #### CHECKER\n",
    "        try:\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test = split_data(\n",
    "                all_preictals[idx], all_interictals[idx], train_ratio, ch_picks, t)\n",
    "\n",
    "\n",
    "\n",
    "            # y_train = to_categorical(y_train, num_classes) \n",
    "            # y_valid = to_categorical(y_valid, num_classes)\n",
    "            # y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "            # Reshape & compile model\n",
    "            Xs = (X_train, X_valid, X_test) # pack\n",
    "            with strategy.scope(): # multi-gpu\n",
    "                Xs_reshaped, model = engine(Xs, learning_rate, batch_size)\n",
    "\n",
    "            X_train, X_valid, X_test = Xs_reshaped # unpack\n",
    "            print('out Xs:', X_train.shape, X_valid.shape, X_test.shape)\n",
    "\n",
    "            if idx == 0:\n",
    "                model.summary()\n",
    "                with redirect_stdout(t):\n",
    "                    model.summary()\n",
    "\n",
    "            print('_' * 65)\n",
    "            print(patient)\n",
    "            print('=' * 65)\n",
    "            print('Num total sequences:', len(all_preictals[idx]) + len(all_interictals[idx]))\n",
    "\n",
    "            fname = '{}_{}_{}'.format(engine_name, patient, arr_name)\n",
    "            fpath = os.path.join(run_dir, 'models', fname)\n",
    "\n",
    "            early_stopping_monitor = EarlyStopping(\n",
    "                monitor='binary_accuracy',\n",
    "                mode='max',\n",
    "                patience=patience,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "\n",
    "            # model_checkpoint_monitor = ModelCheckpoint(\n",
    "            #     fpath, monitor='loss', verbose=0, \n",
    "            #     save_best_only=True, save_weights_only=True, \n",
    "            #     mode='auto', save_freq=batch_size*2**4,\n",
    "            # )\n",
    "\n",
    "            # performance_cbk = PerformanceVisualizationCallback(\n",
    "            #       model=model,\n",
    "            #       validation_data=(X_valid, y_valid),\n",
    "            #       image_dir=image_dir,\n",
    "            # )\n",
    "            # tboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            #     log_dir=run_dir,\n",
    "            #     histogram_freq=1,\n",
    "            #     update_freq=100\n",
    "            #     #profile_batch=[16, 32]\n",
    "            # )\n",
    "\n",
    "            # Fit model\n",
    "            hist = model.fit(X_train, y_train, \n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  verbose=1,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  validation_split=0.2,\n",
    "                  callbacks = [early_stopping_monitor]#, model_checkpoint_monitor]#, tboard_callback],# performance_cbk]\n",
    "            )\n",
    "\n",
    "            val_loss, val_acc = model.evaluate(X_valid, y_valid, verbose=1, batch_size=batch_size)\n",
    "            best_val_loss = min(hist.history['val_loss'])\n",
    "            best_val_acc = max(hist.history['val_binary_accuracy'])\n",
    "            all_loss.append(val_loss)\n",
    "            all_acc.append(val_acc)\n",
    "\n",
    "            print('Valid loss:', round(val_loss, 4), '|',\n",
    "                  'Valid accuracy:', round(val_acc,4))\n",
    "            print('Best valid loss:', round(best_val_loss,4), '|',\n",
    "                  'Best valid accuracy:', round(best_val_acc,4)) # all accuracy is binary accuracy\n",
    "\n",
    "            # Check\n",
    "            y_pred = model.predict(X_test).ravel()\n",
    "            y_pred_class = np.argmax(y_pred) #necessary for binary?\n",
    "\n",
    "            nn_fpr_keras, nn_tpr_keras, nn_thresholds_keras = roc_curve(y_test, y_pred)\n",
    "            auc_keras = auc(nn_fpr_keras, nn_tpr_keras)\n",
    "            all_auc_keras.append(auc_keras)\n",
    "            auc_score = roc_auc_score(y_test,y_pred)\n",
    "            print('AUC Score:', round(auc_score,4))\n",
    "\n",
    "            # Outputs\n",
    "            curr_scores = [patient, round(auc_score,4), round(val_acc,4), round(val_loss,4)]\n",
    "            # cw.writerow(curr_scores)\n",
    "\n",
    "            # TEST\n",
    "            global g_model, g_X_train, g_y_train, g_X_test, g_y_test\n",
    "            g_model, g_X_train, g_y_train, g_X_test, g_y_test = model, X_train, y_train, X_test, y_test\n",
    "\n",
    "#             # Visualization\n",
    "#             # loss, acc plot\n",
    "#             fig, loss_ax = plt.subplots()\n",
    "#             acc_ax = loss_ax.twinx()\n",
    "\n",
    "#             loss_ax.plot(hist.history['loss'], 'y', label = 'train loss')\n",
    "#             loss_ax.plot(hist.history['val_loss'], 'r', label = 'val loss')\n",
    "#             loss_ax.set_ylim([0,1])\n",
    "\n",
    "#             acc_ax.plot(hist.history['binary_accuracy'], 'b', label = 'train accuracy')\n",
    "#             acc_ax.plot(hist.history['val_binary_accuracy'], 'g', label = 'val accuracy')\n",
    "\n",
    "#             loss_ax.set_xlabel('epoch')\n",
    "#             loss_ax.set_ylabel('loss')\n",
    "#             acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "#             loss_ax.legend(loc = 'upper left')\n",
    "#             acc_ax.legend(loc = 'lower left')\n",
    "\n",
    "#             fname = patient + '_loss_acc_' + run_name + '.png'\n",
    "#             fpath = os.path.join(image_dir, fname)\n",
    "#             fig.savefig(fpath)\n",
    "#             plt.clf()\n",
    "\n",
    "            # # auc plot (all)\n",
    "            # plt.figure(2) # all patients\n",
    "            # plt.plot(nn_fpr_keras, nn_tpr_keras, marker='.', label='{} (auc = {})'.format(patient, round(auc_keras,4)) )\n",
    "            \n",
    "#             print()\n",
    "          \n",
    "\n",
    " \n",
    "\n",
    "     # plot & visualize model summary \n",
    "            # if patient == 'SNUCH01' and ch_picks=='all':\n",
    "            #     fname = run_name + '.png'\n",
    "            #     fpath = os.path.join(image_dir, fname)\n",
    "                #plot_model(model, to_file=fpath, show_shapes=True, show_layer_names=True) # CHECKER\n",
    "            \n",
    "                # # keras-visualizer\n",
    "                # fname = engine_name + '_' + run_name + '_graphics.png'\n",
    "                # fpath = os.path.join(image_dir, fname)\n",
    "                # visualizer(model, filename=fpath, format='png', view=False)\n",
    "            \n",
    "            # ## XAI (structured: channels)\n",
    "            # # shap plot issues: https://github.com/slundberg/shap/issues/153\n",
    "            # # compute SHAP values\n",
    "            # explainer = shap.DeepExplainer(model, X_train)\n",
    "            # shap_values = explainer.shap_values(X_test)\n",
    "            # print('======shap_values shape:', shap_values.shape)\n",
    "            \n",
    "            \n",
    "#             # SHAP Global Interpretation\n",
    "#             plt.figure(10)\n",
    "#             shap.summary_plot(shap_values[0], plot_type = 'bar', feature_names = all_ch, \n",
    "#                               show=False, matplotlib=True)\n",
    "#             fname = 'shap_global_' + run_name + '.png'\n",
    "#             fpath = os.path.join(image_dir, fname)\n",
    "#             plt.savefig(fpath, format = \"png\", dpi=150, bbox_inches='tight')\n",
    "#             plt.clf()\n",
    "            \n",
    "#             # SHAP Local Interpretation\n",
    "# #             shap.initjs()\n",
    "# # shap.force_plot(explainer.expected_value[0].np(), shap_values[0][0], features = all_ch)\n",
    "\n",
    "# #             shap.decision_plot(explainer.expected_value[0].numpy(), shap_values[0][0], features = test_data.iloc[0,:], feature_names = all_ch)\n",
    "    \n",
    "# #             shap.plots._waterfall.waterfall_legacy(explainer.expected_value[0].numpy(), shap_values[0][0], feature_names = all_ch)\n",
    "            \n",
    "#             ## XAI (unstructured)\n",
    "            \n",
    "    \n",
    "                       \n",
    "#             # # Profiler\n",
    "#             # model = ct.convert(model)\n",
    "#             # profile = model_profiler(model, batch_size) # measure model profile\n",
    "#             # print(profile)\n",
    "#             # print(profile, file=t)\n",
    "\n",
    "#             # Clear mem\n",
    "#             del model\n",
    "#             tf.keras.backend.clear_session() # may not work. Safety catch with cuda.close() in main()\n",
    "            \n",
    "            \n",
    "        \n",
    "        except ValueError:\n",
    "            raise # TEST # CHECKER\n",
    "            \n",
    "#             # print('Error with {}, skipping...'.format(patient) )\n",
    "#             # print('Error with {}, skipping...'.format(patient), file=t)\n",
    "#             # continue\n",
    "\n",
    "#     print('_' * 65)\n",
    "\n",
    "    # avg_auc_keras = np.mean(all_auc_keras)\n",
    "    # print(\"Avg. auc for all patients:\", round(avg_auc_keras,4))\n",
    "    # print(\"Avg. acc for all patients:\",  round(np.mean(all_acc),4))\n",
    "    # avg_scores = ['Avg', round(avg_auc_keras,4), round(np.mean(all_acc),4), round(np.mean(all_loss),4)]\n",
    "    # cw.writerow(avg_scores)\n",
    "    \n",
    "    # plt.figure(2) # all auc plot\n",
    "    # plt.legend(loc='best')\n",
    "    # fname = 'all_auc' + run_name + '.png'\n",
    "    # fpath = os.path.join(image_dir, fname)\n",
    "    # plt.savefig(fpath)\n",
    "    # plt.clf()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "723f04f8-804b-459f-88bf-11c489b2eb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________________________________________________\n",
      "=================================================================\n",
      "Dataset: seg05m_seq30s\n",
      "=================================================================\n",
      "\n",
      "20220503_143959_BSDCNN_relu_seg05m_seq30s_all\n",
      "Num patients: 1\n",
      "Cnt equivalency: True\n",
      "Num devices: 2\n",
      "Batch size: 64\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "Initiated bsdcnn.py...\n",
      "in Xs: (3697, 21, 3000) (792, 21, 3000) (793, 21, 3000)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got (21, 3000, 1).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:211\u001b[0m, in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m   shape \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1218\u001b[0m, in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensorShape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:771\u001b[0m, in \u001b[0;36mTensorShape.__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;241m=\u001b[39m [as_dimension(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dims_iter]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:771\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;241m=\u001b[39m [\u001b[43mas_dimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dims_iter]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:716\u001b[0m, in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 716\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:197\u001b[0m, in \u001b[0;36mDimension.__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m   \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;167;43;01mTypeError\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDimension value must be integer or None or have \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    199\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43man __index__ method, got \u001b[39;49m\u001b[38;5;132;43;01m{!r}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Dimension value must be integer or None or have an __index__ method, got (21, 3000, 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m## clear VRAM \u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m#cuda.close()\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#########################################################################################\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     run_name \u001b[38;5;241m=\u001b[39m timestamp \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m engine_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m arr_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m ch_picks\n\u001b[1;32m     37\u001b[0m     run_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(outputs_dir, run_name)\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mignition\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch_picks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mignition\u001b[0;34m(arr_name, run_name, run_dir, engine_name, engine, ch_picks)\u001b[0m\n\u001b[1;32m     12\u001b[0m patients, all_preictals, all_interictals \u001b[38;5;241m=\u001b[39m fetcher(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(arr_dir, arr_name))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#print(*map(len, [patients, all_preictals, all_interictals])) # test\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodeler\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_preictals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_interictals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch_picks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m t\u001b[38;5;241m.\u001b[39mclose()\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mmodeler\u001b[0;34m(arr_name, patients, all_preictals, all_interictals, t, cw, run_name, run_dir, engine_name, engine, ch_picks)\u001b[0m\n\u001b[1;32m     40\u001b[0m Xs \u001b[38;5;241m=\u001b[39m (X_train, X_valid, X_test) \u001b[38;5;66;03m# pack\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope(): \u001b[38;5;66;03m# multi-gpu\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     Xs_reshaped, model \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m X_train, X_valid, X_test \u001b[38;5;241m=\u001b[39m Xs_reshaped \u001b[38;5;66;03m# unpack\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout Xs:\u001b[39m\u001b[38;5;124m'\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape, X_valid\u001b[38;5;241m.\u001b[39mshape, X_test\u001b[38;5;241m.\u001b[39mshape)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mbsdcnn_relu\u001b[0;34m(Xs, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m Xs_out, input_shape \u001b[38;5;241m=\u001b[39m shaper(Xs)    \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# pick 1 model\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mb1_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m opt_adam \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39mlearning_rate, beta_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, beta_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.999\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-08\u001b[39m, decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     41\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mopt_adam, \n\u001b[1;32m     42\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     43\u001b[0m              )\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mb1_mlp\u001b[0;34m(input_shape, batch_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mb1_mlp\u001b[39m(input_shape, batch_size):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# https://www.researchgate.net/publication/349293613_Binary_Single-Dimensional_Convolutional_Neural_Network_for_Seizure_Prediction\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.25\u001b[39m))\n\u001b[1;32m     94\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(Dense(batch_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:456\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:193\u001b[0m, in \u001b[0;36mSequential.add\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    190\u001b[0m batch_shape, dtype \u001b[38;5;241m=\u001b[39m training_utils\u001b[38;5;241m.\u001b[39mget_input_shape_and_dtype(layer)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_shape:\n\u001b[1;32m    192\u001b[0m   \u001b[38;5;66;03m# Instantiate an input layer.\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[43minput_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_input\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;66;03m# This will build the current layer\u001b[39;00m\n\u001b[1;32m    196\u001b[0m   \u001b[38;5;66;03m# and create the node connecting the current layer\u001b[39;00m\n\u001b[1;32m    197\u001b[0m   \u001b[38;5;66;03m# to the input layer we just created.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m   layer(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_layer.py:295\u001b[0m, in \u001b[0;36mInput\u001b[0;34m(shape, batch_size, name, dtype, sparse, tensor, ragged, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease provide to Input either a `shape`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    291\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or a `tensor` argument. Note that \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    292\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`shape` does not include the batch \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    293\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m \u001b[43mInputLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_layer_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Return tensor including `_keras_history`.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# Note that in this case train_output and test_output are the same pointer.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m outputs \u001b[38;5;241m=\u001b[39m input_layer\u001b[38;5;241m.\u001b[39m_inbound_nodes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutput_tensors\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_layer.py:149\u001b[0m, in \u001b[0;36mInputLayer.__init__\u001b[0;34m(self, input_shape, batch_size, dtype, input_tensor, sparse, name, ragged, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m graph \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mget_graph()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m--> 149\u001b[0m   input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaceholder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_input_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m      \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m      \u001b[49m\u001b[43mragged\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mragged\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_input_shape \u001b[38;5;241m=\u001b[39m batch_input_shape\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:1087\u001b[0m, in \u001b[0;36mplaceholder\u001b[0;34m(shape, ndim, dtype, sparse, name, ragged)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     x \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(tensor_spec_to_placeholder, type_spec,\n\u001b[1;32m   1085\u001b[0m                            expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1086\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1087\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaceholder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:3026\u001b[0m, in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m   3023\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.placeholder() is not compatible with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3024\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager execution.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaceholder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:6674\u001b[0m, in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   6672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6673\u001b[0m   shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 6674\u001b[0m shape \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6675\u001b[0m _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m _op_def_library\u001b[38;5;241m.\u001b[39m_apply_op_helper(\n\u001b[1;32m   6676\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlaceholder\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, shape\u001b[38;5;241m=\u001b[39mshape, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   6677\u001b[0m _result \u001b[38;5;241m=\u001b[39m _outputs[:]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:213\u001b[0m, in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    211\u001b[0m   shape \u001b[38;5;241m=\u001b[39m tensor_shape\u001b[38;5;241m.\u001b[39mas_shape(v)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError converting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to a TensorShape: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg_name, e))\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError converting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to a TensorShape: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg_name,\n\u001b[1;32m    216\u001b[0m                                                                   e))\n",
      "\u001b[0;31mTypeError\u001b[0m: Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got (21, 3000, 1)."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ignition(arr_name, run_name, run_dir, engine_name, engine, ch_picks):\n",
    "    os.makedirs(run_dir, exist_ok=False)\n",
    "    t = open(os.path.join(run_dir, 'model_run_' + run_name + '.txt'), 'w')\n",
    "    c = open(os.path.join(run_dir, 'model_scores_' + run_name + '.csv'), 'w')\n",
    "    cw = csv.writer(c)\n",
    "    header = ['patient', 'auc', 'acc', 'loss']\n",
    "    cw.writerow(run_name.split('_'))\n",
    "    cw.writerow(header)\n",
    "\n",
    "    print(run_name)\n",
    "    print(run_name, file=t)\n",
    "    patients, all_preictals, all_interictals = fetcher(os.path.join(arr_dir, arr_name))\n",
    "    #print(*map(len, [patients, all_preictals, all_interictals])) # test\n",
    "    modeler(arr_name, patients, all_preictals, all_interictals, t, cw, run_name, run_dir, engine_name, engine, ch_picks)\n",
    "    print('\\n\\n')\n",
    "\n",
    "    t.close()\n",
    "    c.close()\n",
    "\n",
    "def main():\n",
    "    cuda.close() # clear VRAM\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    for arr_name in os.listdir(arr_dir): # for all datasets (of differing seg_len, if any)\n",
    "        # filter out ghosts, and pick desired dataset:\n",
    "        if not arr_name.startswith('.') and 'seg05m' in arr_name:\n",
    "            print()\n",
    "            print('_' * 65)\n",
    "            print('=' * 65)\n",
    "            print('Dataset:', arr_name)\n",
    "            print('=' * 65)\n",
    "            print()\n",
    "            \n",
    "            for engine_name, engine in zip(engine_names, engines):   \n",
    "                for ch_picks in ch_picks_list:\n",
    "                    run_name = timestamp + '_' + engine_name + '_' + arr_name + '_' + ch_picks\n",
    "                    run_dir = os.path.join(outputs_dir, run_name)\n",
    "                    ignition(arr_name, run_name, run_dir, engine_name, engine, ch_picks)\n",
    "                    print()\n",
    "                print()\n",
    "            print()\n",
    "                    \n",
    "    ## clear VRAM \n",
    "    #cuda.close()\n",
    "    \n",
    "    # cuda.select_device(0)\n",
    "    # cuda.close()\n",
    "    # cuda.select_device(1)\n",
    "    # cuda.close()\n",
    "    \n",
    "\n",
    "#########################################################################################\n",
    "main() # call main\n",
    "# penultimate line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af71486c-9d16-43a0-b273-5cc93000e69a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minput_shape\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_shape' is not defined"
     ]
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a50c5a-5a00-4082-9cac-1e8b6753b070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf48b7-bfcd-4a9f-924e-96da06dd08b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56cfe3-fdd7-4167-9003-2deef3f6f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1221c2-ccd9-442a-aebe-c882f22d55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2a8e4-b0f1-4058-b1e6-af9ee074387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # class label list\n",
    "# class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "#                'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "# # example image for each class\n",
    "# images_dict = dict()\n",
    "# for i, l in enumerate(y_train):\n",
    "#   if len(images_dict)==10:\n",
    "#     break\n",
    "#   if l not in images_dict.keys():\n",
    "#     images_dict[l] = x_train[i].reshape((32, 32,3))\n",
    "# images_dict = dict(sorted(images_dict.items()))\n",
    "    \n",
    "# # example image for each class for test set\n",
    "# x_test_dict = dict()\n",
    "# for i, l in enumerate(y_test):\n",
    "#   if len(x_test_dict)==10:\n",
    "#     break\n",
    "#   if l not in x_test_dict.keys():\n",
    "#     x_test_dict[l] = x_test[i]\n",
    "    \n",
    "# # order by class\n",
    "# x_test_each_class = [x_test_dict[i] for i in sorted(x_test_dict)]\n",
    "# x_test_each_class = np.asarray(x_test_each_class)\n",
    "\n",
    "# # Compute predictions\n",
    "# predictions = model.predict(x_test_each_class)\n",
    "# predicted_class = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb961ea-5cdb-4a6e-a2b2-2ada45508091",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHAP image interpretation\n",
    "# set up to pass through BatchNorm\n",
    "shap.explainers._deep.deep_tf.op_handlers[\"AddV3\"] = shap.explainers._deep.deep_tf.passthrough\n",
    "shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough\n",
    "\n",
    "# select backgroud for shap\n",
    "background = g_X_train[np.random.choice(g_X_train.shape[0], 1000, replace=False)]\n",
    "\n",
    "# DeepExplainer to explain predictions of the model\n",
    "explainer = shap.DeepExplainer(g_model, background)\n",
    "\n",
    "# compute shap values\n",
    "shap_values = explainer.shap_values(g_X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628afe80-3ea0-4281-9355-e53f64211c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04cd055-18c1-4e2c-841f-c2eb77a37a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_y_test[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2c848-55ed-472e-a262-fa71359d7802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef53839-5e61-4292-85c2-72f68e9f51ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edae89-bb79-45d1-9e3d-20a1a0a7b209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea7c62-0580-4661-be91-7d6bab6b9b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute SHAP values\n",
    "explainer = shap.DeepExplainer((g_model.layers[0].input, g_model.layers[-1].output), g_X_train[:100])\n",
    "shap_values = explainer.shap_values(g_X_test[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d69138-0600-4bd6-90e5-1c67187fe7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b51ac-68ea-432d-b838-6530084e8476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SHAP Global Interpretation\n",
    "# plt.figure(10)\n",
    "# shap.summary_plot(shap_values[0], plot_type = 'bar', feature_names = all_ch, \n",
    "#                   show=False, matplotlib=True)\n",
    "# fname = 'shap_global_' + run_name + '.png'\n",
    "# fpath = os.path.join(image_dir, fname)\n",
    "# plt.savefig(fpath, format = \"png\", dpi=150, bbox_inches='tight')\n",
    "# plt.clf()\n",
    "\n",
    "# # SHAP Local Interpretation\n",
    "# #             shap.initjs()\n",
    "# # shap.force_plot(explainer.expected_value[0].np(), shap_values[0][0], features = all_ch)\n",
    "\n",
    "# #             shap.decision_plot(explainer.expected_value[0].numpy(), shap_values[0][0], features = test_data.iloc[0,:], feature_names = all_ch)\n",
    "\n",
    "# #             shap.plots._waterfall.waterfall_legacy(explainer.expected_value[0].numpy(), shap_values[0][0], feature_names = all_ch)\n",
    "\n",
    "# ## XAI (unstructured)\n",
    "\n",
    "\n",
    "\n",
    "# # # Profiler\n",
    "# # model = ct.convert(model)\n",
    "# # profile = model_profiler(model, batch_size) # measure model profile\n",
    "# # print(profile)\n",
    "# # print(profile, file=t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eeg]",
   "language": "python",
   "name": "conda-env-eeg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
